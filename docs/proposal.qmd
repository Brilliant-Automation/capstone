---
title: "Intelligent Monitoring and Maintenance Prediction System for Industrial Equipment"
author: "Samuel Adetsi, Mu Ha, Cheng Zhang, Michael Hewlett"
format:
  pdf:
    toc: true
    toc-depth: 2
    number-sections: true
    fig-pos: 'H'
execute:
  echo: false
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: setup
#| include: false

# Load required libraries
library(tidyverse)
library(knitr)
library(kableExtra)
```

\newpage

# Executive Summary

Brilliant Automation, a leader in industrial automation solutions in
Shanghai, China, has engaged our team to develop an intelligent
monitoring and predictive maintenance system for a limestone processing machine.
This project will leverage advanced data analytics and machine learning
to transform sensor data into actionable insights, supporting early
fault detection and optimized maintenance for key machinery.

# Introduction

Brilliant Automation specializes in advanced monitoring and control
systems for manufacturing and processing plants. One of their clients runs a
limestone mine and they have installed high-frequency vibration and temperature sensors
on three devices to monitor device health and preempt breakdowns with predictive
maintenance. The three devices are a tube mill, conveyor belt, and high-temperature fan. 
Figures 1-3 below show the devices and their sensor placements.

![Sensor placements on Tube Mills](images/proposal_machinery_sensor_1.jpg){#fig-sensor-placement-1}

![Sensor placements on Belt Conveyors](images/proposal_machinery_sensor_2.jpg){#fig-sensor-placement-2}

![Sensor placements on High-Temperature Fans](images/proposal_machinery_sensor_3.jpg){#fig-sensor-placement-3}

The purpose of this project is to enhance Brilliant Automation's current 
predictive maintenance capabilities. They have asked us to develop a machine learning
model to predict machine health and a dashboard to display those predictions and select
sensor data.

## Context and Need

Brilliant Automation currently uses MatLab to generate ratings of device health
from sensor data, but MatLab uses proprietary formula and algorithms to generate
those ratings. Brilliant Automation's clients require more transparency on how the
ratings are generated, so they have engaged our team to train interpretable machine
machine learning models to replace their current MatLab implementation. 

## Core Challenges

Our project addresses several key challenges in industrial maintenance:

1.  Converting complex sensor readings into meaningful maintenance
    indicators
2.  Building transparent and reliable prediction models for equipment
    health evaluation
3.  Creating a dashboard that shares key elements with their current Matlab 
    dashboard

## Key Goals

We aim to achieve the following:

1.  Data Analysis and Understanding:
    -   Map relationships across different sensor data
    -   Identify patterns in equipment behavior
    -   Analyze vibration signatures and their implications
2.  Predictive Modeling:
    -   Develop transparent prediction systems
    -   Enable early fault detection
    -   Provide clear reasoning for predictions
3.  Dashboard Development:
    -   Recreate the charts currently used in MatLab

## Project Outputs

Our team will deliver the following key outputs:

-   **Machine Learning Model for Device Ratings:**\
    A machine learning model will be developed for each device to predict 
    health ratings based on sensor data. The goal is for predictions to
    closely match the ratings provided by MatLab so that the model can replace
    MatLab in generating those ratings.

-   **Dashboard:**\
    A dashboard will be created to display device health
    ratings and related analytics. Brilliant Automation has specified what
    elements the dashboard requires, like specific charts that are standard in
    the industry.

-   **Final Report:**\
    Brilliant Automation has request a brief report documenting the rationale behind
    our model selection, feature engineering, and other decisions. They have emphasized
    that this report is to be light weight rather than comprehensive.

# Technical Approach

## Data Overview

We have input and output data from Apr 1 to Apr 15, 2025. The data
consists of 3 devices, summarized below.

```{r}
#| label: data-summary-table
#| tbl-cap: "Measurement System Overview"

data_summary <- data.frame(
  Equipment = c("Tube Mill", "Belt Conveyor #8", "High-Temperature Fan #1"),
  Sensor_Points = c("6 locations", "4 locations", "5 locations"),
  Sensor_Data = c("5-second intervals", "5-second intervals", "5-second intervals"),
  Device_Ratings = c("20-minute intervals", "20-minute intervals", "20-minute intervals")
)

kable(data_summary, format = "latex", booktabs = TRUE, longtable = FALSE) %>%
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    font_size = 9
  )
```

### Input Sensor Data

-   Four key parameters are measured by the sensors at each location, summarized below.
-   These sensor readings are collected at 5-second intervals.

```{r}
#| label: input-data-summary
#| tbl-cap: "Input Data Summary"

sensor_info <- data.frame(
  `Sensor Data` = c("Low Frequency Acceleration", 
                    "High Frequency Acceleration", 
                    "Vibration Velocity Z (z-axis)", 
                    "Temperature"),
  `What It Does` = c("Tracks slow vibrations", 
                     "Tracks fast vibrations", 
                     "Tracks vibration strength vertically", 
                     "Monitors component heat levels"),
  `Why It's Important` = c("Detects alignment issues", 
                           "Detect friction issues", 
                           "Detect system damage", 
                           "Helps prevent overheating")
)

kable(sensor_info, format = "latex", booktabs = TRUE, longtable = FALSE) %>%
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    font_size = 9
  )
```

### Output Device Ratings

-   The system generates 15 device health and status ratings summarized in the table below.
-   Device ratings are produced every 20 minutes.
-   These ratings are generated by a proprietary Matlab program running
    on the machines. The calculation process is a black box:
    Brilliant Automation does not have access to the internal logic or
    algorithms used to derive these ratings.
-   The ratings are out of 100 with the following qualitative scores:

1.  Above 80: Healthy
2.  60 to 79: Usable
3.  30 to 59: Warning
4.  Below 30: Fault

```{r}
#| label: device-output-rating-description
#| tbl-cap: "Device Output Rating Descriptions"

device_ratings <- data.frame(
  `Device Rating` = c(
    "alignment_status", "bearing_lubrication", "crest_factor", "electromagnetic_status", 
    "fit_condition", "kurtosis_opt", "rms_10_25khz", "rms_1_10khz", 
    "rotor_balance_status", "rubbing_condition", "velocity_rms", "peak_value_opt"
  ),
  Description = c(
    "Alignment of conveyor components",
    "Lubrication level in bearings",
    "Ratio of peak amplitude to RMS value",
    "Condition of motor's electromagnetic field",
    "Accuracy of component fit",
    "Kurtosis of optimized vibration signal",
    "Root mean square amplitude (10–25 kHz)",
    "Root mean square amplitude (1–10 kHz)",
    "Balance of the rotor",
    "Friction between components",
    "Overall vibration severity",
    "Optimized vibration peak value"
  ),
  `Rating (0–100)` = c(
    "0: Misaligned; 100: Perfectly aligned",
    "0: Dry; 100: Fully lubricated",
    "0: Low peaks; 100: Severe peaks",
    "0: Faulty field; 100: Stable field",
    "0: Poor fit; 100: Perfect fit",
    "0: Low kurtosis; 100: High kurtosis",
    "0: High amplitude; 100: Low amplitude",
    "0: High amplitude; 100: Low amplitude",
    "0: Imbalanced; 100: Perfect balance",
    "0: Severe rubbing; 100: No rubbing",
    "0: High vibrations; 100: Minimal",
    "0: Low peak; 100: Severe peak"
  )
)

kable(device_ratings, format = "latex", booktabs = TRUE, longtable = FALSE) %>%
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    font_size = 9
  )
```

## Implementation Strategy

![Overview of the end-to-end data pipeline](images/proposal_data_pipeline.png){#fig-pipeline}

The data pipeline (shown in @fig-pipeline) starts with sensor data stored in the client's
internal database. An employee accesses this data using a remote desktop
and copies it to their local computer.

Next, they upload the data to Google Drive. A student working
on the project downloads the uploaded files from Google Drive to their
own computer.

The student then runs a script to preprocess and transform the data.
This includes cleaning the data, selecting important variables, and
reformatting it so that it can be used effectively by machine learning
models.

After preprocessing, the sensor data is fed into a machine learning
model to predict device ratings.

The model outputs are displayed in a dashboard.

Finally, an emplooyee passed a screenshot of the dashboard to an large 
language model (LLM) along with machine part model codes and a description
of the problem the machine is having. The LLM provides a suggestion of the cause
of the problem. The employee uses this response to create summary reports for stakeholders.

## EDA and Data Processing

Since the data on each device is the same (all devices use the same sensors), we've focused our exploratory data analysis (EDA) on one device (the conveyor belt) for efficiency at the MVP stage of the project.

### Input Features EDA

1.  **Feature Distributions:**

    The histograms in @fig-feature-dist show how each feature varies across the three sensor locations: Gear Reducer, Gearbox First Shaft Input End, and Motor Drive End. Features like High-Frequency Acceleration and Low-Frequency Acceleration Z follow approximately normal distributions, but their centers shift depending on location. Temperature varies widely at the Motor Drive End and shows a bimodal pattern, suggesting two different operating states. Vibration Velocity Z is much higher at the Motor Drive End, possibly indicating wear or imbalance.

    Each feature had maximum values that made it difficult to see the rest of the distributions so for the plots in @fig-feature-dist-rm we removed them. The resulting plots better reflect the general distribution across sensor locations.

![Feature Distributions by Location](images/feature_distributions.png){width=70% #fig-feature-dist}

![Feature Distributions by Location (Max Value Removed)](images/feature_distributions_rm_max.png){width=70% #fig-feature-dist-rm}

1.  **Boxplots for Sensor Parameters:**

    The boxplots in @fig-boxplot reveal the spread and outliers of each feature for different sensor locations. For High-Frequency and Low-Frequency Acceleration, the Motor Drive End tends to show more outliers and wider spread. Temperature is generally higher and more stable in the Gear Reducer and Gearbox locations, while the Motor Drive End has lower and more variable temperatures. Vibration Velocity Z is noticeably higher at the Motor Drive End.
    
    Similar to the histograms, we removed the maximum value from each feature before generating the boxplots in @fig-boxplot-rm for clarity.

![Feature Boxplots by Location](images/feature_box_plot.png){fig-cap="Feature Boxplots by Location" width=70% #fig-boxplot}

![Feature Boxplots by Location (Max Value Removed)](images/feature_box_plot_rm_max.png){width=70% #fig-boxplot-rm}

1.  **Feature Correlation Matrix:**

    The heatmap (@fig-heatmap) shows strong positive correlation between High-Frequency and Low-Frequency Acceleration (r ≈ 0.97), suggesting they measure similar physical behavior. Temperature is negatively correlated with Vibration Velocity Z (r ≈ −0.71), which might point to a trade-off between thermal and mechanical stress. The rest of the features show weak or no meaningful correlation, indicating they capture different aspects of the machine's operation.

![Feature Correlation Heatmap](images/feature_correlation_heatmap.png){fig-cap="Feature Correlation Heatmap" width=70% #fig-heatmap}

### Target Features EDA

1.  **Target Rating Distributions:**

    The histograms of the target ratings (@fig-target-dist) show how each variable is distributed across the dataset. Most targets are skewed toward higher values, suggesting that the equipment is generally operating in good condition. A few targets, such as rubbing condition and rotor balance status, show broader distributions, indicating more variability or potential degradation in those areas. Some ratings also show clustering near specific values, which could reflect consistent patterns in operating conditions or thresholds used in the rating system.

![Target Distributions (Histogram)](images/target_distributions.png){fig-cap="Target Distributions (Histogram)" #fig-target-dist width=70%}


2.  **Boxplots for Target Ratings:**

    The boxplots (@fig-target-boxplot) highlight each target’s range, spread, and presence of outliers. Most targets have a compressed interquartile range near the top of the scale, reinforcing the idea that the machines are typically rated well. However, some targets exhibit longer whiskers and outliers, especially for those measuring physical stress or balance conditions. These variations can highlight which conditions are more prone to fluctuations and may require closer monitoring or more robust prediction models.

![Target Distributions (Boxplot)](images/target_boxplots.png){fig-cap="Target Distributions (Boxplot)" width=70% #fig-target-boxplot}

### Data Preprocessing:

We first combine date and time columns into a single
timestamp. Then we pivot the data so
that each sensor measurement and device rating has its own column. Because the
ratings are recorded every 20 minutes and sensor data every 5 seconds,
each rating is duplicated across the corresponding 5-second intervals to
align the data. Temperature is measured every 10 seconds and does not change
significantly within short time spans, so we use forward fill to handle
missing values. Next we add sensor location as a feature. Lastly, the device column is
dropped since we train a separate model for each device.

## Model Development

The model aims to predict device ratings for each equipment unit, using
sensor data as input. Its details are summarized in the table below.

```{r}
#| label: model-comparison
#| tbl-cap: "Model Comparison Overview"

model_comparison <- data.frame(
  Model = c("Baseline", "Ridge", "PolyRidge (deg 2)", "PolyRidge (deg 5)", 
            "Random Forest", "Neural Network"),
  Complexity = c("Very Low", "Low", "Medium", "High", "Medium–High", "High"),
  Interpretability = c("Perfectly clear", "High", "Medium", "Low", "Low–Medium", "Low"),
  Flexibility = c("None", "Only linear fits", "Simple non-linearities", 
                  "Highly flexible curves", "Arbitrary non-linear", "Very high"),
  Overfit_Risk = c("None", "Low–Medium", "Medium", "High", "Medium", "High"),
  Compute_Cost = c("Minimal", "Fast", "Moderate", "Heavy", "Moderate–High", "Heavy")
)

kable(model_comparison, format = "latex", booktabs = TRUE, longtable = TRUE) %>%
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    font_size = 9
  )
```

## Interactive Dashboard

The interactive dashboard serves as the central interface for
maintenance teams and stakeholders to monitor machine health and
understand sensor behavior in real time. It combines predictive model
outputs and raw sensor readings in a clear, user-friendly layout. At the
top of the dashboard, dropdown filters allow users to select a specific
device and sensor, enabling targeted exploration. The radar charts
visualize device health ratings across multiple metrics, helping teams
quickly assess overall performance. Below and to the right, time-series
and frequency plots show raw sensor data to help identify patterns,
anomalies, or failure signals. This layered design allows users to
connect machine learning predictions with actual sensor behavior.

The dashboard is built to meet industrial standards, as defined by
client specifications. Its layout and visualization types are aligned
with existing operational workflows, making it easy for technicians and
analysts to interpret results. The responsiveness and modularity of the
dashboard ensure it remains scalable for additional sensors or machines
in the future.

# Project Timeline

The project timeline is provided in the table below.

```{r}
#| label: project-timeline
#| tbl-cap: "Project Timeline and Outputs"

library(knitr)
library(kableExtra)

timeline <- data.frame(
  Week = 1:8,
  Stage = c("Project launch, data processing", 
            "Data product MVPs", 
            "Full data test", 
            "Model revision", 
            "Model revision", 
            "Output refinement", 
            "Output refinement", 
            "Final checks"),
  Outputs = c("Wrangled dataset, toy dataset, MDS Proposal presentation",
              "MVP dashboard, MVP models, MDS Proposal report",
              "Cloud computing pipeline, initial results",
              "Engineered features",
              "Engineered features",
              "Final dashboard, final models, MDS draft data product",
              "Final dashboard, final models, MDS presentation",
              "Final report, MDS final data product")
)

kable(timeline, format = "latex", booktabs = TRUE, longtable = FALSE) %>%
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    font_size = 9
  )
```
